Project Notes
09/04/20
To do
. Concatenate dataset loaded from data.txt with wikitext

Notes
What about training and validation datasets?
Are they different for wikitext?
Solution:
	Build the whole dataset before it gets split between training and validation.

Concatenate two datasets
What do we know?
Datasets are wrappers of pyarrow tables.
Pyarrow tables are made of pyarrow arrays.
They have schemas which look like this: column_name:data_type

Are there any built in functions to combine two pyarrow tables?

plan b

We don't need to concatenate datasets. We just need to read the data file and add it to the dataset. 

Where should this happen? 


wikitext_client.extract_knowledge

This is a good spot because that is where it would happen regardless of whether we are using the text file or not. 

In extract knowledge
	If the text file exists, use that.
	Otherwise create one and write to it.  

I am changing wikidata client so that it only writes to a file if one is specified.

I need to fix rewrite of some tests for the wikidata client. It says test is not defined for some reason.

09/09/20
I'm not sure where I left off. I think I need to finish writing TestWikidataClient again.

09/10/20
Yes, I need to finish testing WikidataClient

extract_knowledge expected behavior:
  First we check if the data_file exists. 
  If it does, read the next line of the file and set that equal to x['statement']
  return

Let's check if a file exists. 
Ok, the file exists. But it is empty. We enter the if statement and try to read a line.

We get the 'Posixpath' error because self.data_file is a PosixPath. Does not have readline. 

Is self.datafile supposed to be the file itself or the file path?
ok I set a break point to see if the file is getting opened in the init.

ok so the first breakpoint we hit is extract_knowledge.
That doesn't seem right because we only want to use that function through wikidataclient. As in we shouldn't call the function without first initializing the wikidata client.

Actually I think the problem is that the is_file is returning false. 

Ok, I think the real problem is my tests don't know whether to expect to create a file or to read from a file. The file path is given, so a text file is created. 

Let me look at all the tests and see if they should be changed. 

test_get_wikidata_entity_id

Some functions only get called if there is no file. Like this one.
This test shouldn't need any special treatment.
The test passes.

test_get_entity_from_ranked_phrases
This should be fine
passes

test_extract_knowledge
This one should be split into two tests. With and without a textfile. 

I fixed the first test where it is writing not reading.

Now I need to make a new test where I am reading not writing.
Ok, that test works now. Good :)

That's all the tests so wikidata client is working :)

There are some warnings on the test but I'll allow it.


09/14/20

I think I'm ready to run experiment 1.

EXPERIMENT 1

I train GPT2 from scratch on 1% of the Wikitext dataset. 
First, GPT2 is trained without any statements from the knowledge base. 
    This has already been done.
    We should check this on wandb.

    Ok, the run is still there and looks good. It took 4 hours to train. 
    It would be best to not rerun it right now. 

Second, GPT2 is trained with statements.
    What file is that experiment in?
    Ok, the experiment is in experiment-1/experiment_1_kb.py
    I need to pass it the correct data file. How should that be done?

Ok, I ran the experiment and there is one problem.
The no kb experiment used 10% of the data while the kb experiment only used 1%.
I want to rerun the no kb experiment with only 1% of the data.
I think this will require refactoring the wikitextlm to turn off the kb statements. 

